# src/analyzer_agent/analyzer.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional
import logging
import json
import os
import asyncio

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | ANALYZER | %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="Analyzer Agent API", version="0.1")

# --- í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ëª¨í‚¹/ì‹¤ì œ ì‹¤í–‰ ë¡œì§ ---
# DISABLE_LLM_INTEGRATION=True ì¼ ë•Œ ëª¨í‚¹ í™œì„±í™” (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ë¡œì§ ë°˜ì „)
DISABLE_LLM_INTEGRATION = os.getenv("DISABLE_LLM_INTEGRATION", "True").lower() in ('true', '1', 't')
logger.info(f"âš™ï¸  LLM Integration Status: {'MOCKING' if DISABLE_LLM_INTEGRATION else 'ACTIVE'}")

# --- ë°ì´í„° ëª¨ë¸ (Orchestratorì™€ ê³µìœ ) ---

class DetectRequest(BaseModel):
    timestamp: Any
    namespace: str
    pod_name: str              
    event_type: str
    phase: Optional[str] = None
    container_statuses: Optional[List[Dict[str, Any]]] = None
    reasons: Optional[List[str]] = None
    raw_log_tail: Optional[str] = ""
    describe_snippet: Optional[str] = "" 
    metadata: Optional[Dict[str, Any]] = None
    detection_signature: Optional[str] = None
    class Config:
        extra = 'allow'

class AnalyzeCommandResponse(BaseModel):
    command_type: str # 'read' or 'write'
    command_list: List[str]
    is_risky: bool = False

class InitialAnalyzeRequest(BaseModel):
    task_id: str
    detect_request: DetectRequest

class FinalAnalyzeRequest(BaseModel):
    task_id: str
    detect_request: DetectRequest
    execution_logs: List[Dict[str, Any]]
    rag_results: List[Dict[str, Any]]

# --- ìºì‹œ êµ¬ì¡° ëª¨í‚¹ ---
# TODO: Redisë‚˜ DBë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
COMMAND_CACHE: Dict[str, AnalyzeCommandResponse] = {}


# --- í•µì‹¬ ë¡œì§ ---
def _call_llm_api(req: InitialAnalyzeRequest | FinalAnalyzeRequest, is_final: bool) -> AnalyzeCommandResponse:
    """
    ì‹¤ì œ LLM API í˜¸ì¶œ ë¡œì§ (ë¯¸êµ¬í˜„)
    """
    # TODO: ì‹¤ì œ LLM API í˜¸ì¶œ ë¡œì§ êµ¬í˜„
    logger.warning(f"âš ï¸ [LLM CALL] Actual LLM API call is not yet implemented. Task: {req.task_id}")
    
    # ì„ì‹œë¡œ ëª¨í‚¹ ê²°ê³¼ë¥¼ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” LLM ì‘ë‹µì„ íŒŒì‹±í•´ì•¼ í•¨)
    if is_final:
        return _mock_llm_generate_final_command(req)
    else:
        return _mock_llm_generate_initial_command(req)

def _mock_llm_generate_initial_command(req: InitialAnalyzeRequest) -> AnalyzeCommandResponse:
    """
    ë¬¸ì œ ì›ì¸ íŒŒì•…ì„ ìœ„í•œ ì¦ê±°ìˆ˜ì§‘ ëª…ë ¹ì–´ ìƒì„± (LLM ëª¨í‚¹)
    """
    pod_name = req.detect_request.pod_name
    namespace = req.detect_request.namespace
    
    # 2. LLMìœ¼ë¡œ ëª…ë ¹ì–´ ìƒì„± (í•˜ë“œì½”ë”© ëª¨í‚¹)
    logger.info(f"ğŸ§  [LLM MOCK] Generating initial command for {pod_name}")
    
    commands = [
        f"kubectl describe pod {pod_name} -n {namespace}",
        f"kubectl logs {pod_name} -n {namespace} --tail=50"
    ]
    
    response = AnalyzeCommandResponse(
        command_type="read",
        command_list=commands,
        is_risky=False
    )
    
    return response

def _mock_llm_generate_final_command(req: FinalAnalyzeRequest) -> AnalyzeCommandResponse:
    """
    ìµœì¢… ë¬¸ì œ í•´ê²° ëª…ë ¹ì–´ ìƒì„± (LLM ëª¨í‚¹)
    """
    pod_name = req.detect_request.pod_name
    namespace = req.detect_request.namespace
    
    # 2. LLMìœ¼ë¡œ ëª…ë ¹ì–´ ìƒì„± (í•˜ë“œì½”ë”© ëª¨í‚¹)
    logger.info(f"ğŸ§  [LLM MOCK] Generating final command for {pod_name}")
    
    # RAG ê²°ê³¼ì—ì„œ "OOMKilled" ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ë©”ëª¨ë¦¬ ì¦ê°€ ëª…ë ¹ì„ ìƒì„±í•˜ëŠ” ëª¨í‚¹ ë¡œì§
    rag_content = " ".join([r.get("content", "") for r in req.rag_results])
    is_oom_issue = "OOMKilled" in rag_content
    
    if is_oom_issue:
        commands = [
            f"kubectl delete pod {pod_name} -n {namespace}" # Pod ì‚­ì œ í›„ ì¬ìƒì„± ìœ ë„ (OOMKilled í•´ê²°ì„ ìœ„í•œ ì„ì‹œ ì¡°ì¹˜)
        ]
        is_risky = True # delete ëª…ë ¹ì€ ìœ„í—˜í•˜ë‹¤ê³  ê°€ì •
    else:
        # ê·¸ ì™¸ì˜ ê²½ìš°, ì•ˆì „í•œ ëª…ë ¹ì–´ ëª¨í‚¹
        commands = [
            "echo 'No write command generated for this scenario.'"
        ]
        is_risky = False
        
    response = AnalyzeCommandResponse(
        command_type="write",
        command_list=commands,
        is_risky=is_risky
    )
    
    return response

def _get_generated_command(req: InitialAnalyzeRequest | FinalAnalyzeRequest, is_final: bool) -> AnalyzeCommandResponse:
    """ëª¨í‚¹ ì—¬ë¶€ì— ë”°ë¼ ëª…ë ¹ì–´ ìƒì„± í•¨ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
    
    # ìºì‹œ í‚¤ ìƒì„±
    pod_name = req.detect_request.pod_name
    signature = req.detect_request.detection_signature
    cache_key = f"{'final' if is_final else 'initial'}:{signature}:{pod_name}"
    
    # 1. ìºì‹œ í™•ì¸ (ëª¨í‚¹/ì‹¤ì œ ëª¨ë‘ ì ìš©)
    if cache_key in COMMAND_CACHE:
        logger.info(f"ğŸ’¾ [CACHE HIT] Task {req.task_id} command found in cache.")
        return COMMAND_CACHE[cache_key]

    # 2. ëª¨í‚¹/ì‹¤ì œ ì‹¤í–‰ ì„ íƒ
    if DISABLE_LLM_INTEGRATION:
        cmd = _mock_llm_generate_final_command(req) if is_final else _mock_llm_generate_initial_command(req)
    else:
        cmd = _call_llm_api(req, is_final)
        
    # 3. ìºì‹œì— ì €ì¥ (ì‹¤ì œë¡œëŠ” signature ê¸°ë°˜ìœ¼ë¡œ ì €ì¥í•´ì•¼ í•¨)
    # COMMAND_CACHE[cache_key] = cmd # í˜„ì¬ëŠ” ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì„ ì²˜ë¦¬
    
    return cmd

# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@app.post('/analyze/initial', response_model=AnalyzeCommandResponse)
async def analyze_initial(req: InitialAnalyzeRequest):
    """ì¦ê±° ìˆ˜ì§‘ì„ ìœ„í•œ ì´ˆê¸° ëª…ë ¹ì–´ ìƒì„±"""
    logger.info(f"ğŸ“© [POST /analyze/initial] Task {req.task_id} received for initial analysis.")
    await asyncio.sleep(1) 
    return _get_generated_command(req, is_final=False)

@app.post('/analyze/final', response_model=AnalyzeCommandResponse)
async def analyze_final(req: FinalAnalyzeRequest):
    """ìµœì¢… ë¬¸ì œ í•´ê²° ëª…ë ¹ì–´ ìƒì„±"""
    logger.info(f"ğŸ“© [POST /analyze/final] Task {req.task_id} received for final analysis.")
    await asyncio.sleep(1)
    return _get_generated_command(req, is_final=True)

@app.get('/health')
async def health():
    return {"status": "ok"}

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8034, log_level="info")
